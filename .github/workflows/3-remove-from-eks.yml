name: '3-Destroy Apps from EKS'

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: 'AWS Region (e.g., us-east-1)'
        required: true
        default: 'us-east-1'
      cluster_name:
        description: 'EKS Cluster Name (e.g., staging-eks-demo)'
        required: true
        default: 'staging-eks-demo'

jobs:
  destroy-from-eks:
    name: 'Destroy ALB and Applications'
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.aws_region }}

#--------------------------- 1. Update kube-config-----------------------------------------




      - name: Update Kubeconfig
        run: |
          aws eks update-kubeconfig --region ${{ github.event.inputs.aws_region }} --name ${{ github.event.inputs.cluster_name }}
          echo "CORRECT:---> Kubeconfig updated for cluster: ${{ github.event.inputs.cluster_name }}"
#-------------------------------------------------------------------------------------------------------------------------------




# --------------------------2. Delete the ArgoCD Ingress to trigger ALB deletion------------------------------
# Checking if the ALB is deleted:
    # If deleted then, OUTPUT:"CORRECT:------> Success! The ALB is no longer found in AWS and is confirmed deleted."
    #If not deleted then, It will keep looping for 5 minutes , updating every 15 seconds
    # ALTERNATIVE SITUATION: There can be a possibility that the ALB is already deleted , then
        # OUTPUT : "------XXXX--- Could not find an ALB hostname from the Ingress. It might have been deleted already ---XXXX----------------"


      - name: Delete Ingress and Wait for ALB Deletion
        run: |
          # Fetching ALB HOSTNAME 
          ALB_HOSTNAME=$(kubectl get ingress argocd-server-ingress -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")

          echo "****-------------------Deleting ArgoCD Ingress to trigger ALB deletion--------------****"
          kubectl delete -f kubernetes-manifests/argocd-ingress.yml -n argocd --ignore-not-found=true

          if [ -z "$ALB_HOSTNAME" ]; then
            echo "------XXXX--- Could not find an ALB hostname from the Ingress. It might have been deleted already ---XXXX----------------"
            echo "Skipping polling and proceeding."
          else
            echo "Actively polling to confirm deletion of ALB: $ALB_HOSTNAME"
            
            SECONDS=0
            TIMEOUT=300 # 5-minute timeout

            while [ $SECONDS -lt $TIMEOUT ]; do
              # Check if an ALB with the specific DNS name still exists
              ALB_ARN=$(aws elbv2 describe-load-balancers --query "LoadBalancers[?DNSName=='$ALB_HOSTNAME'].LoadBalancerArn" --output text)
              
              if [ -z "$ALB_ARN" ]; then
                echo "CORRECT:------> Success! The ALB is no longer found in AWS and is confirmed deleted."
                break # Exit the loop successfully
              fi
              
              echo "****-------------------Waiting for ALB to be deleted... ($SECONDS/$TIMEOUT seconds)--------------****"
              sleep 15
              SECONDS=$((SECONDS + 15))
            done

            if [ $SECONDS -ge $TIMEOUT ]; then
              echo "ERROR: Timed out after 5 minutes waiting for ALB to be deleted."
              exit 1
            fi
          fi

          echo "Sleeping for 60 seconds to make sure that the Target Group and SGs deleted"
          sleep 60
#-------------------------------------------------------------------------------------------------------------------------------





#----------------------------------------- 3. Uninstall ArgoCD ---------------------------------------------------------------------


      - name: Uninstall ArgoCD
        run: |
          echo "xxxx-------------- Uninstalling ArgoCD Helm chart----------------X"
          helm uninstall argocd -n argocd || echo "ArgoCD not found, skipping."
#-------------------------------------------------------------------------------------------------------------------------------



#--------------------------------------- 4. Uninstall AWS Load Balancer Controller -----------------------------------------------



      - name: Uninstall AWS Load Balancer Controller
        run: |
          echo "xxxx---------Uninstalling AWS Load Balancer Controller Helm chart.----------xxxx"
          helm uninstall aws-load-balancer-controller -n kube-system || echo "AWS Load Balancer Controller not found, skipping."
#-------------------------------------------------------------------------------------------------------------------------------




#-------------------------------------  5. Detach and Delete IAM Policy ----------------------------------------
# NOTE: We now delete the policy because it is custom-created by the deploy workflow
# and is specific to this application stack.

      - name: Detach and Delete IAM Policy for ALB Controller
        run: |
          echo "---------> Starting detachment and deletion of IAM policy..."
          POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='AWSLoadBalancerControllerIAMPolicy'].Arn" --output text)

          if [ -z "$POLICY_ARN" ]; then
            echo "IAM Policy 'AWSLoadBalancerControllerIAMPolicy' not found. Skipping cleanup."
            exit 0
          fi

          echo "Found policy to clean up: $POLICY_ARN"
          
          NODE_GROUP_NAME=$(aws eks list-nodegroups --cluster-name ${{ github.event.inputs.cluster_name }} --query "nodegroups[0]" --output text)
          if [ -z "$NODE_GROUP_NAME" ]; then
            echo "No node groups found in cluster. Skipping detachment."
          else
            NODE_ROLE_ARN=$(aws eks describe-nodegroup --cluster-name ${{ github.event.inputs.cluster_name }} --nodegroup-name "$NODE_GROUP_NAME" --query "nodegroup.nodeRole" --output text)
            NODE_ROLE_NAME=$(basename $NODE_ROLE_ARN)

            echo "Detaching policy $POLICY_ARN from role $NODE_ROLE_NAME"
            aws iam detach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN || echo "Failed to detach policy, it might have already been detached."
            sleep 5 # Allow a moment for the detachment to propagate
          fi
          
          echo "Deleting policy $POLICY_ARN"
          aws iam delete-policy --policy-arn $POLICY_ARN || echo "Failed to delete policy, it might have been already deleted."

          echo "COMPLETED:-------> Policy cleanup process complete."
#-------------------------------------------------------------------------------------------------------------------------------


      - name: Completion Step
        run: |
          echo "=========================================================================================="
          echo "COMPLETED:-->>EKS cluster cleanup is complete."
          echo "You can now safely run the Terraform workflow to destroy the remaining infrastructure."
          echo "=========================================================================================="
#-------------------------------------------------------------------------------------------------------------------------------