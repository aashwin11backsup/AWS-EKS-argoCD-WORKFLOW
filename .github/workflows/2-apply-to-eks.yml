name: '2-Deploy Apps to EKS'

on:
  workflow_dispatch:
    inputs:
      aws_region:
        description: 'AWS Region (e.g., us-east-1)'
        required: true
        default: 'us-east-1'
      cluster_name:
        description: 'EKS Cluster Name (e.g., staging-eks-demo)'
        required: true
        default: 'staging-eks-demo'

jobs:
  deploy-to-eks:
    name: 'Deploy ALB Controller and ArgoCD'
    runs-on: ubuntu-latest

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ github.event.inputs.aws_region }}

      - name: Verify AWS Authentication
        run: |
          echo "=========================================="
          echo "âœ… Verifying AWS Authentication"
          echo "=========================================="
          CALLER_IDENTITY=$(aws sts get-caller-identity)
          echo "Caller Identity: $CALLER_IDENTITY"
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "âœ… Authenticated as Account: $ACCOUNT_ID"
          echo "=========================================="

      # Step to install eksctl
      - name: Install eksctl
        run: |
          echo "=========================================="
          echo "ðŸ“¦ Installing eksctl"
          echo "=========================================="
          curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
          sudo mv /tmp/eksctl /usr/local/bin
          EKSCTL_VERSION=$(eksctl version)
          echo "âœ… eksctl installed: $EKSCTL_VERSION"
          echo "=========================================="
          
#-------------------------------------------------------------------------------------------------------------------------------
      # 1. Connect to the EKS Cluster
      - name: Update Kubeconfig
        run: |
          echo "=========================================="
          echo "ðŸ”Œ Connecting to EKS Cluster"
          echo "=========================================="
          echo "Region: ${{ github.event.inputs.aws_region }}"
          echo "Cluster: ${{ github.event.inputs.cluster_name }}"
          
          aws eks update-kubeconfig --region ${{ github.event.inputs.aws_region }} --name ${{ github.event.inputs.cluster_name }}
          echo "âœ… Kubeconfig updated"
          
          echo ""
          echo "ðŸ“Š Checking cluster nodes:"
          kubectl get nodes -o wide
          
          NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
          echo ""
          echo "âœ… Found $NODE_COUNT node(s) in the cluster"
          echo "=========================================="
          sleep 3
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
      # 2. Create IAM Policy for ALB Controller
      - name: Ensure IAM Policy for ALB Controller
        id: ensure_policy
        run: |
          echo "=========================================="
          echo "ðŸ“‹ Ensuring IAM Policy for ALB Controller"
          echo "=========================================="
          POLICY_NAME="AWSLoadBalancerControllerIAMPolicy"
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          echo "Account ID: $ACCOUNT_ID"
          echo "Policy Name: $POLICY_NAME"

          # Check if the policy already exists
          EXISTING_POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='$POLICY_NAME'].Arn" --output text)

          if [ -n "$EXISTING_POLICY_ARN" ]; then
            echo "âœ… Policy already exists: $EXISTING_POLICY_ARN"
            POLICY_ARN=$EXISTING_POLICY_ARN
          else
            echo "âš ï¸  Policy not found. Creating new policy..."
            curl -sSfL -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
            echo "Downloaded policy document"
            
            POLICY_ARN=$(aws iam create-policy --policy-name $POLICY_NAME --policy-document file://iam-policy.json --query 'Policy.Arn' --output text)
            echo "âœ… Created policy: $POLICY_ARN"
          fi
          
          echo "policy_arn=$POLICY_ARN" >> "$GITHUB_OUTPUT"
          echo ""
          echo "âœ… Final Policy ARN: $POLICY_ARN"
          echo "=========================================="
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
      # 3a. Associate OIDC Provider (Required for IRSA)
      - name: Associate IAM OIDC Provider
        run: |
          echo "=========================================="
          echo "ðŸ”— Associating OIDC Provider with Cluster"
          echo "=========================================="
          
          eksctl utils associate-iam-oidc-provider \
            --cluster=${{ github.event.inputs.cluster_name }} \
            --region=${{ github.event.inputs.aws_region }} \
            --approve
          
          echo "âœ… OIDC Provider associated"
          echo "=========================================="
          sleep 3
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
           # 3b. Create IAM Service Account for ALB Controller (IRSA)
      - name: Create IAM Service Account for ALB Controller
        env:
          POLICY_ARN: ${{ steps.ensure_policy.outputs.policy_arn }}
        run: |
          echo "=========================================="
          echo "ðŸ”‘ Creating IRSA for ALB Controller"
          echo "=========================================="
          echo "Cluster: ${{ github.event.inputs.cluster_name }}"
          echo "Region: ${{ github.event.inputs.aws_region }}"
          echo "Policy ARN: $POLICY_ARN"
          
          ROLE_NAME="AmazonEKSLoadBalancerControllerRole-${{ github.event.inputs.cluster_name }}"
          echo "IAM Role Name: $ROLE_NAME"
          
          echo ""
          echo "ðŸ§¹ Cleaning up any existing IRSA resources..."
          
          # Delete using eksctl (this removes the CloudFormation stack)
          eksctl delete iamserviceaccount \
            --cluster=${{ github.event.inputs.cluster_name }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --region=${{ github.event.inputs.aws_region }} || echo "âš ï¸  No existing service account to delete (this is OK)"
          
          echo ""
          echo "â³ Waiting 10 seconds for cleanup to complete..."
          sleep 10
          
          echo ""
          echo "ðŸ†• Creating fresh IRSA resources..."
          eksctl create iamserviceaccount \
            --cluster=${{ github.event.inputs.cluster_name }} \
            --namespace=kube-system \
            --name=aws-load-balancer-controller \
            --role-name "$ROLE_NAME" \
            --attach-policy-arn=$POLICY_ARN \
            --approve \
            --region=${{ github.event.inputs.aws_region }}
          
          echo ""
          echo "âœ… IAM Role and Service Account created"
          echo ""
          echo "ðŸ“‹ Verifying Service Account:"
          kubectl get sa aws-load-balancer-controller -n kube-system -o yaml
          
          echo ""
          echo "ðŸ” Checking annotation:"
          ROLE_ARN=$(kubectl get sa aws-load-balancer-controller -n kube-system -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}')
          if [ -n "$ROLE_ARN" ]; then
            echo "âœ… Service Account has role annotation: $ROLE_ARN"
          else
            echo "âŒ ERROR: Service Account missing role annotation!"
            exit 1
          fi
          echo "=========================================="
          sleep 3
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
      # 4. Install AWS Load Balancer Controller
      - name: Install AWS Load Balancer Controller
        run: |
          echo "=========================================="
          echo "ðŸš€ Installing AWS Load Balancer Controller"
          echo "=========================================="
          
          VPC_ID=$(aws eks describe-cluster --name ${{ github.event.inputs.cluster_name }} --query "cluster.resourcesVpcConfig.vpcId" --output text)
          echo "VPC ID: $VPC_ID"
          
          echo ""
          echo "Adding Helm repo..."
          helm repo add eks https://aws.github.io/eks-charts
          helm repo update
          
          echo ""
          echo "Installing Helm chart..."
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            --namespace kube-system \
            --set clusterName=${{ github.event.inputs.cluster_name }} \
            --set serviceAccount.create=false \
            --set serviceAccount.name=aws-load-balancer-controller \
            --set vpcId=$VPC_ID \
            --set region=${{ github.event.inputs.aws_region }}

          echo ""
          echo "â³ Waiting 60 seconds for controller to deploy..."
          sleep 30
          
          echo ""
          echo "ðŸ“Š Checking deployment status:"
          kubectl get deployment aws-load-balancer-controller -n kube-system
          
          echo ""
          echo "ðŸ“¦ Checking pods:"
          kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o wide
          
          POD_STATUS=$(kubectl get pods -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller -o jsonpath='{.items[0].status.phase}' 2>/dev/null || echo "NotFound")
          echo ""
          if [ "$POD_STATUS" == "Running" ]; then
            echo "âœ… ALB Controller pod is Running"
          else
            echo "âš ï¸  ALB Controller pod status: $POD_STATUS"
          fi
          
          echo ""
          echo "ðŸ“ Recent controller logs:"
          kubectl logs -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller --tail=30 || true
          echo "=========================================="
          sleep 3
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
      # 5. Install ArgoCD
      - name: Install ArgoCD
        run: |
          echo "=========================================="
          echo "ðŸ™ Installing ArgoCD"
          echo "=========================================="
          
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update
          
          helm upgrade --install argocd argo/argo-cd \
            --namespace argocd \
            --create-namespace \
            --set server.service.type=ClusterIP \
            --set server.extraArgs={--insecure}
          
          echo ""
          echo "â³ Waiting 30 seconds for ArgoCD pods to start..."
          sleep 30
          
          echo ""
          echo "ðŸ“¦ Checking ArgoCD pods:"
          kubectl get pods -n argocd -o wide
          
          READY_PODS=$(kubectl get pods -n argocd --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l || echo "0")
          TOTAL_PODS=$(kubectl get pods -n argocd --no-headers 2>/dev/null | wc -l || echo "0")
          echo ""
          echo "âœ… $READY_PODS/$TOTAL_PODS pods are Running"
          echo "=========================================="
          sleep 3
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
      # 6. Expose ArgoCD Server using Ingress
      - name: Expose ArgoCD Server
        run: |
          echo "=========================================="
          echo "ðŸŒ Creating Ingress for ArgoCD"
          echo "=========================================="
          
          echo "Waiting for ArgoCD server to be ready..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s
          echo "âœ… ArgoCD server is ready"
          
          echo ""
          echo "Applying Ingress manifest from argocd/ directory..."
          kubectl apply -f argocd/
          
          echo ""
          echo "ðŸ“‹ Checking Ingress resource:"
          kubectl get ingress -n argocd
          echo "=========================================="
          sleep 3
#-------------------------------------------------------------------------------------------------------------------------------

#-------------------------------------------------------------------------------------------------------------------------------
      # 7. Get the ALB DNS Name for ArgoCD
      - name: Get Ingress URL
        run: |
          echo "=========================================="
          echo "â³ Waiting for ALB to be provisioned..."
          echo "=========================================="
          echo "This may take 2-3 minutes..."
          echo ""
          
          kubectl wait --for=jsonpath='{.status.loadBalancer.ingress[0].hostname}' ingress/shared-main-ingress -n argocd --timeout=60s || echo "âš ï¸  Timeout reached, checking status..."

          echo ""
          echo "ðŸ“‹ Ingress Details:"
          kubectl describe ingress shared-main-ingress -n argocd || true
          
          echo ""
          echo "=========================================="
          ALB_HOSTNAME=$(kubectl get ingress shared-main-ingress -n argocd -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          
          if [ -n "$ALB_HOSTNAME" ]; then
            echo "âœ… ArgoCD URL is ready:"
            echo "   http://$ALB_HOSTNAME"
          else
            echo "âš ï¸  ALB not ready yet. Check again in a few minutes with:"
            echo "   kubectl get ingress shared-main-ingress -n argocd"
          fi
          echo "=========================================="

          echo ""
          echo "=========================================="
          echo "ðŸ”‘ ArgoCD Login Credentials"
          echo "=========================================="
          echo "Username: admin"
          echo -n "Password: "
          kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" 2>/dev/null | base64 -d || echo "Secret not found"
          echo ""
          echo "=========================================="
#-------------------------------------------------------------------------------------------------------------------------------